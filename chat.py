import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
from utils import apply_lora, load_lora_weight
from LoRA.lora_config import LoraConfig

# ä¿å­˜å¤šç§ loraï¼Œç”¨äºåŠ¨æ€åŠ è½½
lora_dict = {
    "base": {
        "path": None,
        "weight": None,
    },
    "cute": {
        "path": "/mnt/workspace/lora_llava_finetuned/cute.bin",
        "weight": None,
    },
    "literary": {
        "path": "/mnt/workspace/lora_llava_finetuned/literary.bin",
        "weight": None,
    }
}

# base model è·¯å¾„
MODEL_PATH = "/mnt/workspace/llava-interleave-qwen-0.5b-hf"

# lora é…ç½®ï¼Œå¿…é¡»ä¸ lora è®­ç»ƒæ—¶ä¸€è‡´
lora_config = LoraConfig()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def main():
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    model = LlavaForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
    )
    model.generation_config.pad_token_id = 151645
    processor = AutoProcessor.from_pretrained(MODEL_PATH)

    # åŠ å…¥ LoRA ç»“æ„
    model = apply_lora(model, lora_config.target_modules, lora_config.rank, lora_config.alpha)
    model = model.to(device)
    # print('model:')
    # print(model)

    # åŠ è½½ LoRA æƒé‡åˆ°å†…å­˜
    for each in lora_dict.values():
        if each["path"]:
            each["weight"] = torch.load(each["path"])

    model.eval()
    
    # åˆå§‹å›¾ç‰‡è·¯å¾„
    image_path = "food.png"  # ç¡®ä¿å›¾ç‰‡æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹
    raw_image = Image.open(image_path)

    while True:
        user_input = input("ğŸ¤—ï¼š")
        if user_input == "exit":
            break

        if user_input == "image":
            img_filename = input("è¯·è¾“å…¥å›¾ç‰‡æ–‡ä»¶åï¼š")  
            try:
                raw_image = Image.open(img_filename)
                print(f"å›¾ç‰‡'{img_filename}'åŠ è½½æˆåŠŸ")
                language_quality = input("ğŸ¤—ï¼š") 
            except Exception as e:
                print(f"å›¾ç‰‡åŠ è½½å¤±è´¥ï¼š{e}")

        if user_input == "lora":
            # åŠ è½½æŒ‡å®šçš„ lora æƒé‡
            lora_name = input("è¯·è¾“å…¥è¦æ›¿æ¢çš„é£æ ¼ï¼š")
            if lora_name not in lora_dict:
                print(f"åä¸º {lora_name} çš„ lora æƒé‡ä¸å­˜åœ¨")
                continue

            load_lora_weight(model, lora_dict[lora_name]["weight"])
            continue

        # æ„é€ æ¨¡å‹è¾“å…¥
        conversation = [{
            "role": "user",
            "content": [
                {"type": "text", "text": user_input},
                {"type": "image"},
            ],
        }]
        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
        inputs = processor(
            images=raw_image,
            text=prompt,
            return_tensors='pt'
        ).to(0)

        # æ¨¡å‹æ¨ç†
        output = model.generate(
            **inputs,
            max_new_tokens=200,
        )

        # å°†è¾“å‡ºçš„ token è§£ç ä¸ºæ–‡æœ¬
        output_text = processor.decode(output[0], skip_special_tokens=True)

        # åªè¾“å‡º â€˜ASSISTANTâ€™ åé¢çš„æ–‡æœ¬
        print(f'ğŸ¤–ï¼š{output_text.split("assistant")[-1].strip()}')


if __name__ == "__main__":
    main()
