# ğŸ˜»mini-LLaVA: An multimodal AI assistantğŸ¤– for creating cuteğŸ¥° and literaryğŸ§ style content based on LoRA

The backbone of mini-LLaVA adopts **Qwen2-0.5B** as the language model and **SigLIP** as the visual feature extractor for cross-modal connection. We adopted the **LoRA (rank = 8)** to simulate full fine tuning and combined with **8-bit quantization** deployment. In the blind test, the style consistency received **79% user preference**.

# 1. Main Environments
The following are the **external libraries** used in the project, as well as the versions adopted by the author during runtime.
```
1.  torch - 2.6.0
2.  transformers - 4.49.0
3.  PIL(Pillow) - 11.1.0
4.  requests - 2.22.0
5.  openai - 1.65.2
6.  tqdm - 4.67.1
```

# 2. File Structure
```
mnt/workspace/
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ chat.py
â”œâ”€â”€ LoRA/
â”‚   â”œâ”€â”€ lora_config.py
â”‚   â””â”€â”€ lora_layer.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cute.json
â”‚   â”œâ”€â”€ literary.json
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ read_data.py
â”‚   â””â”€â”€ make_data.py
â”‚
â”œâ”€â”€ llava-interleave-qwen-0.5b-hf/...
â”‚
â””â”€â”€ lora_llava_finetuned/
    â”œâ”€â”€ cute.bin
    â””â”€â”€ literary.bin
```
1. The `LoRA` folder contains the custom-made LoRA component and the parameter configuration file.  
2. The `data` folder contains triplet training data in cute and literary styles generated by DeepSeek-V3, as well as some data preprocessing files.  
3. The `lora_llava_finetuned` folder contains the fine-tuned weights for the two styles.  
4. The `llava-interleave-qwen-0.5b-hf` folder contains the pre-trained LLaVA-Qwen-0.5B model migrated from Modelscope to the local environment.

# 3. Download from Modelscope
If you want to learn more about **LLaVA-Qwen-0.5B**, please [click here to visit Modelscope.](https://www.modelscope.cn/models/llava-hf/llava-interleave-qwen-0.5b-hf)
You can execute the following command in the terminal to load the pre-trained model to your local machine.
```
# Install git-lfs to facilitate the download of large files.
apt install git-lfs
git lfs install

# Download weights
git clone https://www.modelscope.cn/llava-hf/llava-interleave-qwen-0.5b-hf.git

# If you want to use a more powerful 7B model instead of the 0.5B one, you can download the following pre-trained weights.
git clone https://www.modelscope.cn/llava-hf/llava-1.5-7b-hf.git
```
# 4. Chat with AI assistantğŸ¤–
1. Load the image you want to generate text for to your local machine. For this example, we will use this image about [food](https://th.bing.com/th/id/R.b3a7697d2793ba094a861d546c31190d?rik=NevOIW4XmkUuMA&riu=http%3a%2f%2fseopic.699pic.com%2fphoto%2f50069%2f5445.jpg_wh1200.jpg&ehk=wuLPicg%2b9wXz8QAwp%2fAVFBtJQ6loBUiVfQZu2bbZODA%3d&risl=&pid=ImgRaw&r=0) for testing. ![food](https://th.bing.com/th/id/R.b3a7697d2793ba094a861d546c31190d?rik=NevOIW4XmkUuMA&riu=http%3a%2f%2fseopic.699pic.com%2fphoto%2f50069%2f5445.jpg_wh1200.jpg&ehk=wuLPicg%2b9wXz8QAwp%2fAVFBtJQ6loBUiVfQZu2bbZODA%3d&risl=&pid=ImgRaw&r=0)
2. In the `chat.py` file, modify the `image_path` variable to the name of this image. 
3. Run `chat.py` and input "**lora**". Then, choose the style you want (**base**, **cute**, or **literary**). 
4. Ask the model a question (currently, it only supports **Chinese**), for example: "**è¿™å¼ å›¾ç‰‡æ˜¯ä¸€äº›ç¾é£Ÿï¼Œå¸®æˆ‘ç”Ÿæˆä¸€æ®µæœ‹å‹åœˆçš„æ–‡æ¡ˆ**" 
5. You will receive the result:
```bash
ğŸ¤—ï¼šlora
è¯·è¾“å…¥è¦æ›¿æ¢çš„é£æ ¼ï¼šcute
ğŸ¤—ï¼šè¿™å¼ å›¾ç‰‡æ˜¯ä¸€äº›ç¾é£Ÿï¼Œå¸®æˆ‘ç”Ÿæˆä¸€æ®µæœ‹å‹åœˆçš„æ–‡æ¡ˆ
ğŸ¤–ï¼šå“å‘€å‘€ï½ä»Šå¤©ç»™å¤§å®¶åˆ†äº«ä¸€ä¸ªè¶…çº§å¥½åƒçš„ç¾é£Ÿä½“éªŒå“¦ï¼ğŸ˜ ä½ çœ‹é‚£ä¸²ä¸²çš„çƒ¤è‚‰ï¼Œå¤–çš®é‡‘é»„é…¥è„†ï¼Œé‡Œé¢è‚‰è´¨é²œå«©å¤šæ±ï¼Œç®€ç›´æ˜¯äººé—´ç¾å‘³å‘¢ï¼ğŸ˜ è€Œä¸”å“¦ï¼Œæ¯ä¸€å£éƒ½èƒ½æ„Ÿå—åˆ°çƒ¤è‚‰çš„é¦™é¦™ç”œç”œï¼Œç®€ç›´æ˜¯è®©äººå¿ä¸ä½æƒ³å†æ¥ä¸€æ¬¡å‘¢ï¼ğŸ˜ è¿˜æœ‰é‚£ä¸²ä¸²çš„è”¬èœï¼Œç»¿ç»¿çš„ï¼Œçœ‹ç€å°±è®©äººæƒ³åƒï¼Œè€Œä¸”æ¯ä¸€å£éƒ½èƒ½æ„Ÿå—åˆ°è”¬èœçš„æ–°é²œå’Œç¾å‘³ï¼Œç®€ç›´æ˜¯è®©äººåœä¸ä¸‹æ¥ï¼ğŸ˜ è¿™äº›ç¾é£Ÿçš„æ­é…ï¼Œç®€ç›´æ˜¯è®©äººåœä¸ä¸‹æ¥ï¼Œè®©äººå¿ä¸ä½æƒ³è¦å†æ¥ä¸€æ¬¡å‘¢ï¼ğŸ˜ å¿«æ¥å’Œæˆ‘ä¸€èµ·ï¼Œäº«å—è¿™ä»½ç¾å‘³å§ï¼ğŸ˜˜
ğŸ¤—ï¼šlora
è¯·è¾“å…¥è¦æ›¿æ¢çš„é£æ ¼ï¼šliterary
ğŸ¤—ï¼šè¿™å¼ å›¾ç‰‡æ˜¯ä¸€äº›ç¾é£Ÿï¼Œå¸®æˆ‘ç”Ÿæˆä¸€æ®µæœ‹å‹åœˆçš„æ–‡æ¡ˆ
ğŸ¤–ï¼šåœ¨è¿™å¼ å›¾ç‰‡ä¸­ï¼Œæ¯ä¸€ç‰‡è‚‰éƒ½åƒæ˜¯æ—¶é—´çš„ç¢ç‰‡ï¼Œè½»è½»æŠšæ‘¸ç€å‘³è•¾ã€‚é‚£çº¢è‰²çš„è‚‰ï¼Œä»¿ä½›æ˜¯å²æœˆçš„å°è®°ï¼Œå¸¦ç€ä¸€è‚¡è‚¡æ¸©æš–çš„å…‰èŠ’ï¼Œè®©äººä¸ç¦æƒ³è¦ä¸€å£å’¬ä¸‹ã€‚ç»¿è‰²çš„è”¬èœï¼Œåƒæ˜¯å¤§è‡ªç„¶çš„é¦ˆèµ ï¼Œä¸ºè¿™é¡¿æ™šé¤å¢æ·»äº†ä¸€æŠ¹ç”Ÿæœºã€‚è€Œé‚£ç‰‡ç»¿è‰²çš„è”¬èœï¼Œåƒæ˜¯æ—¶é—´çš„è§è¯è€…ï¼Œé™é™åœ°èººåœ¨é‚£é‡Œï¼Œç­‰å¾…ç€è¢«æˆ‘ä»¬ç”¨å®ƒæ¥è®²è¿°ä¸€ä¸ªæ•…äº‹ã€‚è¿™é¡¿æ™šé¤ï¼Œä¸ä»…ä»…æ˜¯å£ä¸­çš„äº«å—ï¼Œæ›´æ˜¯å‘³è•¾ä¸å¿ƒçµçš„å¯¹è¯ã€‚è®©æˆ‘ä»¬ä¸€èµ·ï¼Œç”¨è¿™é¡¿æ™šé¤ï¼Œå»æ„Ÿå—ç”Ÿæ´»çš„ç¾å¥½ï¼Œå»ä½“éªŒç¾é£Ÿçš„æ— é™å¯èƒ½ã€‚#ç¾é£Ÿ #ç”Ÿæ´» #å‘³è•¾ä¸å¿ƒçµçš„å¯¹è¯
```
6. If you want to change the image mid-conversation, you can input â€œ**image**â€, then enter the name of the image you want to inquire about from your local machine.
```bash
ğŸ¤—ï¼šlora
è¯·è¾“å…¥è¦æ›¿æ¢çš„é£æ ¼ï¼šcute
ğŸ¤—ï¼šimage
è¯·è¾“å…¥å›¾ç‰‡æ–‡ä»¶åï¼šSU7_Ultra.png
å›¾ç‰‡'SU7_Ultra.png'åŠ è½½æˆåŠŸ
ğŸ¤—ï¼šè¿™å¼ å›¾ç‰‡æ˜¯å°ç±³æ±½è½¦çš„å‘å¸ƒæµ·æŠ¥ï¼Œå¸®æˆ‘ç”Ÿæˆä¸€æ®µå°çº¢ä¹¦çš„æ–‡æ¡ˆ
ğŸ¤–ï¼šå“å‘€å‘€ï½ä»Šå¤©è¦ç»™å¤§å®¶ç§è‰ä¸€ä¸ªè¶…çº§é…·ç‚«çš„æ±½è½¦å“¦ï¼ğŸš—ğŸ’¨ å°ç±³æ±½è½¦çš„SUVç³»åˆ—ï¼ŒçœŸçš„æ˜¯è®©äººå¿ä¸ä½æƒ³è¦å¼€ä¸Šä¸€åœˆå‘¢ï¼é»‘è‰²çš„è½¦èº«åŠ ä¸Šçº¢è‰²çš„æ¡çº¹è£…é¥°ï¼Œç®€ç›´å°±åƒæ˜¯ä»ç«¥è¯ä¸–ç•Œé‡Œèµ°å‡ºæ¥çš„æ±½è½¦ä¸€æ ·ï¼ğŸš—ğŸ’¨ è¿™è¾†SUVä¸ä»…å¤–è§‚é…·ç‚«ï¼Œè€Œä¸”åŠ¨åŠ›å¼ºåŠ²ï¼Œæ— è®ºæ˜¯æ—¥å¸¸é€šå‹¤è¿˜æ˜¯å‡ºè¡—ï¼Œéƒ½èƒ½è®©ä½ æ„Ÿå—åˆ°æ»¡æ»¡çš„è‡ªä¿¡ï¼ğŸš—ğŸ’¨ è€Œä¸”å“¦ï¼Œå®ƒçš„ä»·æ ¼ä¹Ÿéå¸¸äº²æ°‘ï¼Œåªè¦10000å…ƒï¼Œå°±èƒ½æ‹¥æœ‰è¿™æ ·ä¸€è¾†è¶…çº§é…·ç‚«çš„æ±½è½¦ï¼Œæ˜¯ä¸æ˜¯è¶…çº§å¸å¼•äººå‘¢ï¼ŸğŸ˜ å¿«æ¥è¯•è¯•çœ‹å§ï¼Œä¿è¯è®©ä½ çˆ±ä¸é‡Šæ‰‹ï¼ğŸ˜˜
```
# 5. Train your own featured AI assistantğŸ¤–
If you want to train an AI assistant in a style you like or not a copywriting type, you can do it yourself by following the steps below!
1. In the `data/make_data.py` file, modify the `prompt_template` and use your own `api_key`. If you have a large number of data entries, don't forget to modify `tot_num` to the number of triplet training data you want to generate. 
2. `make_data.py` will generate a JSON file at the end, which will be used as a dataset for `train.py`. In `train.py`, you can manually adjust the batch size, learning rate, and epoch for each training session. The resulting model weights will be saved in the `lora_llava_finetuned` folder. Don't forget to modify the weights file in `chat.py`!
